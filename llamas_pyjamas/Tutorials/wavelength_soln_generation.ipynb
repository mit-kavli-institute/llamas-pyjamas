{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import cloudpickle\n",
    "from astropy.io import fits\n",
    "from llamas_pyjamas.config import CALIB_DIR, OUTPUT_DIR, LUT_DIR\n",
    "from llamas_pyjamas.constants import idx_lookup\n",
    "from llamas_pyjamas.Flat.flatProcessing import produce_flat_extractions, reduce_flat\n",
    "from llamas_pyjamas.Utils.utils import concat_extractions\n",
    "from llamas_pyjamas.Arc.arcLlamas import arcTransfer\n",
    "import llamas_pyjamas.Extract.extractLlamas as extract\n",
    "from llamas_pyjamas.constants import RED_IDXS, GREEN_IDXS, BLUE_IDXS\n",
    "import llamas_pyjamas.Arc.arcLlamasMulti as arc\n",
    "import llamas_pyjamas.GUI.guiExtract as ge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2289ea",
   "metadata": {},
   "source": [
    "# Generating new wavelength solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7337e5",
   "metadata": {},
   "source": [
    "### 1. selecting the arc file to extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/Users/slh/Documents/Projects/Magellan_dev/LLAMAS/testing/wavelength_soln_comissioning/'\n",
    "\n",
    "arc_filename   = '/Users/slh/Library/CloudStorage/Box-Box/slhughes/Llamas_Commissioning_Data/ut20241202-03/LLAMAS_2024-12-03T17_11_57.326_mef.fits'\n",
    "ge.GUI_extract(arc_filename, output_dir=OUTPUT_DIR)\n",
    "\n",
    "arc_picklename = os.path.join(OUTPUT_DIR, os.path.basename(arc_filename).replace('_mef.fits', '_extract.pkl'))\n",
    "with open(arc_picklename, 'rb') as fp:\n",
    "    batch_data = pickle.load(fp)\n",
    "arcdict = extract.ExtractLlamas.loadExtraction(arc_picklename)\n",
    "arcspec, metadata = arcdict['extractions'], arcdict['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.shiftArcX(arc_picklename)\n",
    "shift_picklename = arc_picklename.replace('_extract.pkl', '_extract_shifted.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf71f9",
   "metadata": {},
   "source": [
    "### Extract the appropriate flat field files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_flat_file = '/Users/slh/Library/CloudStorage/Box-Box/slhughes/Llamas_Commissioning_Data/ut20241202-03/LLAMAS_2024-12-02T23_53_52.524_mef.fits'\n",
    "green_flat_file = '/Users/slh/Library/CloudStorage/Box-Box/slhughes/Llamas_Commissioning_Data/ut20241202-03/LLAMAS_2024-12-02T23_48_34.754_mef.fits'\n",
    "blue_flat_file = '/Users/slh/Library/CloudStorage/Box-Box/slhughes/Llamas_Commissioning_Data/ut20241202-03/LLAMAS_2024-12-02T23_48_34.754_mef.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_flat(red_flat_file, RED_IDXS, tracedir=CALIB_DIR, channel='red', save_dir=OUTPUT_DIR)\n",
    "reduce_flat(green_flat_file, GREEN_IDXS, tracedir=CALIB_DIR, channel='green', save_dir=OUTPUT_DIR)\n",
    "reduce_flat(blue_flat_file, BLUE_IDXS, tracedir=CALIB_DIR, channel='blue', save_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b416839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "combined_flat_file = os.path.join(OUTPUT_DIR, 'combined_flat_extractions.pkl')\n",
    "_ex = [os.path.join(OUTPUT_DIR, 'red_extractions_flat.pkl'), os.path.join(OUTPUT_DIR, 'green_extractions_flat.pkl'), os.path.join(OUTPUT_DIR, 'blue_extractions_flat.pkl')]\n",
    "concat_extractions(_ex, combined_flat_file)\n",
    "\n",
    "red_extract = extract.ExtractLlamas.loadExtraction(os.path.join(OUTPUT_DIR, 'red_extractions_flat.pkl'))\n",
    "green_extract = extract.ExtractLlamas.loadExtraction(os.path.join(OUTPUT_DIR, 'green_extractions_flat.pkl'))\n",
    "blue_extract = extract.ExtractLlamas.loadExtraction(os.path.join(OUTPUT_DIR, 'blue_extractions_flat.pkl'))\n",
    "\n",
    "print(combined_flat_file)\n",
    "print(len(red_extract['extractions']))\n",
    "print(len(green_extract['extractions']))\n",
    "print(len(blue_extract['extractions']))\n",
    "\n",
    "import llamas_pyjamas.Extract.extractLlamas as extract # type: ignore\n",
    "\n",
    "flatdict = extract.ExtractLlamas.loadExtraction(combined_flat_file)\n",
    "print(len(flatdict['extractions']))\n",
    "\n",
    "# This sets a reference point to use for relative throughput calculations\n",
    "flatdict['extractions'][10].relative_throughput[5] = 0.96\n",
    "flatdict['extractions'][10].relative_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff49157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "from llamas_pyjamas.constants import idx_lookup\n",
    "import llamas_pyjamas.Extract.extractLlamas as extract\n",
    "\n",
    "def reorder_extraction_to_standard(input_pickle, output_pickle=None):\n",
    "    \"\"\"\n",
    "    Reorder an extraction pickle to match the standard LLAMAS extension ordering.\n",
    "    \n",
    "    Standard order is: red1A, green1A, blue1A, red1B, green1B, blue1B, ...\n",
    "    \n",
    "    Args:\n",
    "        input_pickle: Path to the input extraction pickle\n",
    "        output_pickle: Path for output (default: replaces input with '_reordered.pkl')\n",
    "    \n",
    "    Returns:\n",
    "        Path to the reordered pickle file\n",
    "    \"\"\"\n",
    "    # Load the extraction\n",
    "    data = extract.ExtractLlamas.loadExtraction(input_pickle)\n",
    "    extractions = data['extractions']\n",
    "    metadata = data['metadata']\n",
    "    \n",
    "    # Build a mapping from (channel, bench, side) -> current index\n",
    "    current_mapping = {}\n",
    "    for i, meta in enumerate(metadata):\n",
    "        key = (meta['channel'].lower(), str(meta['bench']), meta['side'].upper())\n",
    "        current_mapping[key] = i\n",
    "    \n",
    "    # Print current ordering for debugging\n",
    "    print(\"Current extension order:\")\n",
    "    for i, meta in enumerate(metadata):\n",
    "        print(f\"  {i}: {meta['channel']} {meta['bench']}{meta['side']} (nfibers={meta['nfibers']})\")\n",
    "    \n",
    "    # Create new lists in standard order\n",
    "    new_extractions = []\n",
    "    new_metadata = []\n",
    "    \n",
    "    # Sort by idx_lookup value to get standard order\n",
    "    sorted_keys = sorted(idx_lookup.keys(), key=lambda k: idx_lookup[k])\n",
    "    \n",
    "    print(\"\\nReordering to standard order:\")\n",
    "    for new_idx, key in enumerate(sorted_keys):\n",
    "        channel, bench, side = key\n",
    "        lookup_key = (channel.lower(), str(bench), side.upper())\n",
    "        \n",
    "        if lookup_key in current_mapping:\n",
    "            old_idx = current_mapping[lookup_key]\n",
    "            new_extractions.append(extractions[old_idx])\n",
    "            new_metadata.append(metadata[old_idx])\n",
    "            print(f\"  {new_idx}: {channel} {bench}{side} <- was index {old_idx}\")\n",
    "        else:\n",
    "            print(f\"  WARNING: {channel} {bench}{side} not found in input!\")\n",
    "    \n",
    "    # Save the reordered extraction\n",
    "    if output_pickle is None:\n",
    "        output_pickle = input_pickle.replace('.pkl', '_reordered.pkl')\n",
    "    \n",
    "    # Create new dict structure (include primary_header if present)\n",
    "    reordered_data = {\n",
    "        'extractions': new_extractions,\n",
    "        'metadata': new_metadata\n",
    "    }\n",
    "    if 'primary_header' in data:\n",
    "        reordered_data['primary_header'] = data['primary_header']\n",
    "    \n",
    "    # Use cloudpickle like the rest of the codebase\n",
    "    with open(output_pickle, 'wb') as f:\n",
    "        cloudpickle.dump(reordered_data, f)\n",
    "    \n",
    "    print(f\"\\nSaved reordered extraction to: {output_pickle}\")\n",
    "    return output_pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b106122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arc.fiberRelativeThroughput(combined_flat_file, shift_picklename)\n",
    "\n",
    "reordered_flat = reorder_extraction_to_standard(combined_flat_file)\n",
    "# Then use the reordered file:\n",
    "arc.fiberRelativeThroughput(reordered_flat, shift_picklename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(combined_flat_file, 'rb') as f:\n",
    "    flat_data = pickle.load(f)\n",
    "    \n",
    "with open(shift_picklename, 'rb') as f:\n",
    "    arc_data = pickle.load(f)\n",
    "\n",
    "# Find the 1B extension and compare\n",
    "for ext, meta in enumerate(flat_data['metadata']):\n",
    "    if meta.get('bench') == 1 and meta.get('side') == 'B':\n",
    "        print(f\"Extension {ext}: {meta.get('bench')}{meta.get('side')} {meta.get('channel')}\")\n",
    "        print(f\"  Flat nfibers: {meta['nfibers']}\")\n",
    "        print(f\"  Arc xshift shape: {arc_data['extractions'][ext].xshift.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=shift_picklename.replace('.pkl','_shifted_tp.pkl')\n",
    "arc.arcSolve(tp, savefile='test_wavelength_solution.pkl', savedir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599b922",
   "metadata": {},
   "source": [
    "### Checking the wavelength solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamas_pyjamas.Utils.utils import check_reference_arc_wavelength_ranges\n",
    "ref = os.path.join(OUTPUT_DIR, 'test_wavelength_solution.pkl')\n",
    "check_reference_arc_wavelength_ranges(ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
